{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/jun99/miniconda3/envs/jun/lib/python3.9/site-packages (4.66.4)\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm # 진행바 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DirectoryLoader + PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pdf 출처 : https://www.koraia.org/default/mp5/sub3.php?sub=03\n",
    "- 자료가 꽤 꾸준히 올라오기도하고, 퀄리티도 좋아서 ai관련해선 좋은 소스임\n",
    "\n",
    "Langchain Document : https://python.langchain.com/docs/how_to/document_loader_directory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Specify the directory containing your PDF files\n",
    "pdf_folder_path = \"/home/jun99/rag/pdfdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\xb0' @1025546: b'F12\\\\n/BaseFont /\\\\xc4\\\\xda\\\\xbe\\\\xee \\\\xb0\\\\xed\\\\xb5\\\\xf1 E 7 ExtraBold\\\\n/Encoding /Identity-H\\\\n\\\\n/DescendantFonts ['\")\n",
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\xb5' @13704199: b'/BaseFont /\\\\xbf\\\\xa1\\\\xbd\\\\xba\\\\xc4\\\\xda\\\\xbe\\\\xee \\\\xb5\\\\xe5\\\\xb8\\\\xb2 8 Heavy\\\\n/Encoding /Identity-H\\\\n\\\\n/DescendantFonts [ 84819'\")\n",
      "100%|██████████| 12/12 [00:54<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.8 s, sys: 784 ms, total: 54.6 s\n",
      "Wall time: 54.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a DirectoryLoader\n",
    "loader = DirectoryLoader(pdf_folder_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader,\n",
    "                         show_progress=True) # 진행바 추가\n",
    "# Load all PDF documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyPDFDirectoryLoader도 있긴한데, 확장성을 위해 DirectoryLoader사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='책임\u0000있는\u0000AI를\u0000위한\u0000기업의\u0000노력과\u0000시사점\n",
      "Corporate Efforts for Responsible AI and Implications\n",
      "경기도 성남시 분당구 대왕판교로 712 번길 22 글로벌 R&D 연구동 (B) 4층\n",
      "Global R&D Center 4F 22 Daewangpangyo-ro 712beon-gil, Bundang-gu, Seongnam-si, Gyeonggi-do\n",
      " \n",
      "www.spri.kr\u0000 ISSN\u00002733-6336' metadata={'source': '/home/jun99/rag/pdfdata/[이슈리포트]_책임 있는 AI를 위한 기업의 노력과 시사점.pdf', 'page': 33}\n"
     ]
    }
   ],
   "source": [
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그런데 일단 documents를 보면, page_content에 뭐랄까 metadata같은 내용들도 있어서 조금 그럼 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) # 딱 페이지 수만큼 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What multithreading?\n",
    "- 여러 파일들을 동시에 병렬적으로 다운받는 기능 : 효율적임! <- 대용량에서\n",
    "- 근데 기본값으론 False인데, 안좋은점이 있는걸까? -> 안좋은건 내 노트북이였고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\xb0' @1025546: b'F12\\\\n/BaseFont /\\\\xc4\\\\xda\\\\xbe\\\\xee \\\\xb0\\\\xed\\\\xb5\\\\xf1 E 7 ExtraBold\\\\n/Encoding /Identity-H\\\\n\\\\n/DescendantFonts ['\")\n",
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\xb5' @13704199: b'/BaseFont /\\\\xbf\\\\xa1\\\\xbd\\\\xba\\\\xc4\\\\xda\\\\xbe\\\\xee \\\\xb5\\\\xe5\\\\xb8\\\\xb2 8 Heavy\\\\n/Encoding /Identity-H\\\\n\\\\n/DescendantFonts [ 84819'\")\n",
      "100%|██████████| 12/12 [01:00<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 s, sys: 3.62 s, total: 1min 2s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# # 멀티스레드 활성화 : 병렬 로드 -> 하드 디스크 성능 이슈로 오히려 더 느려짐...쓰지말자 ㅠㅠ\n",
    "# loader = DirectoryLoader(pdf_folder_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader,\n",
    "#                          show_progress=True, use_multithreading=True) \n",
    "\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 loader쓸 땐, 그냥 Directory로 정해서 쓰면 될 듯하다\n",
    "\n",
    "\n",
    "범용성도 커서, 유지 & 보수도 하기 쉽다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitter\n",
    "- RecursiveCharacterTextSplitter많이 쓰는데, 보통 chunk_size(1000~1500)으로 잡고 overlap은 15~20%로 하는 게 일반적\n",
    "\n",
    "- ref : https://github.com/langchain-ai/langchain/issues/2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(split_docs), type(split_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========0번째 문서========================\n",
      "page_content='ISSUE\u0000REPORT\u0000l\u00002024.08.19\u0000IS-178\n",
      "글로벌\u0000초거대\u0000AI\u0000모델\u0000현황\u0000분석\n",
      "(2020~2023년)\n",
      "An Analysis on the Global Large-scale AI Model(2020-2023)\n",
      "봉강호' metadata={'source': '/home/jun99/rag/pdfdata/(IS-178) 글로벌 초거대 AI 모델 현황 분석(2020~2023년)_SPRi_240819.pdf', 'page': 0}\n",
      "=========1번째 문서========================\n",
      "page_content='이\u0000보고서는\u0000 ｢과학기술정보통신부\u0000 정보통신진흥기금 ｣에서\u0000지원받아\u0000제작한\u0000것으로\u0000\n",
      "과학기술정보통신부의\u0000 공식의견과\u0000다를\u0000수\u0000있습니다.\n",
      "이\u0000보고서의\u0000내용은\u0000연구진의\u0000개인\u0000견해이며,\u0000본\u0000보고서와\u0000관련한\u0000의문\u0000사항\u0000또는\u0000수정·보완할\u0000\n",
      "필요가\u0000있는\u0000경우에는\u0000아래\u0000연락처로\u0000연락해\u0000주시기\u0000바랍니다.\n",
      "소프트웨어정책연구소\u0000 AI정책연구실\n",
      "봉강호\u0000선임연구원\u0000bk91@spri.kr' metadata={'source': '/home/jun99/rag/pdfdata/(IS-178) 글로벌 초거대 AI 모델 현황 분석(2020~2023년)_SPRi_240819.pdf', 'page': 1}\n",
      "=========2번째 문서========================\n",
      "page_content='SPRi\u0000이슈리포트\u0000IS-178 글로벌\u0000초거대\u0000AI\u0000모델\u0000현황\u0000분석(2020~2023년)\n",
      "\u0000\u0000CONTENT\n",
      "\u0000\u0000\u0000\u0000\n",
      "Ⅰ.\u0000서론 P.1\n",
      "Ⅱ.\u0000글로벌\u0000초거대\u0000AI\u0000모델\u0000현황 P.4\n",
      "III.\u0000요약\u0000및\u0000정책적\u0000시사점 P.12\n",
      "참고문헌 P.16' metadata={'source': '/home/jun99/rag/pdfdata/(IS-178) 글로벌 초거대 AI 모델 현황 분석(2020~2023년)_SPRi_240819.pdf', 'page': 2}\n",
      "=========3번째 문서========================\n",
      "page_content='SPRi\u0000이슈리포트\u0000IS-178 글로벌\u0000초거대\u0000AI\u0000모델\u0000현황\u0000분석(2020~2023년)\n",
      "요\u0000약\u0000문\n",
      "인공지능(AI)\u0000기술은\u0000급격한\u0000속도로\u0000발전해왔으며,\u0000특히\u00002020년대에\u0000들어서면서\u0000초거대\u0000AI\u0000\n",
      "모델이\u0000경쟁적으로\u0000등장하고\u0000있다.\u0000여기서\u0000초거대\u0000AI\u0000모델은\u0000대용량\u0000연산\u0000인프라를\u0000바탕으로\u0000\n",
      "방대한\u0000데이터를\u0000학습해\u0000인간처럼\u0000종합적인\u0000인지·판단·추론이\u0000가능해진\u0000‘큰\u0000규모’의\u0000AI\u0000모델을\u0000\n",
      "의미한다.\u0000특정\u0000목적에\u0000따라\u0000개별의\u0000데이터를\u0000수집·학습하여\u0000만들어지는\u0000기존의\u0000일반\u0000AI는\u0000학\n",
      "습된\u0000과업(task)에\u0000한하여\u0000수행이\u0000가능한\u0000반면,\u0000초거대\u0000AI는\u0000더욱\u0000복잡하고\u0000광범위한\u0000분야에서\u0000\n",
      "과업을\u0000수행할\u0000수\u0000있다.\n",
      "본고에서는\u00002020년부터\u00002023년까지\u0000전\u0000세계에\u0000출시된\u0000초거대\u0000AI\u0000모델\u0000현황을\u0000분석하고,\u0000\n",
      "글로벌\u0000기술\u0000동향과\u0000트렌드를\u0000살펴보았다.\u0000구체적으로,\u0000미국\u0000민간\u0000연구단체인\u0000‘EPOCH\u0000AI’가\u0000\n",
      "최근\u0000업데이트(‘24년\u00007월)한\u0000초거대\u0000AI\u0000모델\u0000현황\u0000DB를\u0000통해\u0000데이터를\u0000수집하고,\u00002020년부터\u0000\n",
      "2023년까지\u0000출시된\u0000초거대\u0000AI\u0000모델에\u0000대해\u0000출시년도,\u0000국가,\u0000분야,\u0000과업유형,\u0000개발형태,\u0000개발조직\u0000\n",
      "유형\u0000등의\u0000다양한\u0000기준으로\u0000정리·분석하였다.\u0000우리나라\u0000현황에\u0000대해서도\u0000주목하고,\u0000AI\u0000분야에\u0000\n",
      "대한\u0000정책적\u0000시사점을\u0000도출하였다.\n",
      "Executive\u0000Summary\u0000\n",
      "Artificial\u0000intelligence\u0000(AI)\u0000technology\u0000is\u0000developing\u0000at\u0000a\u0000rapid\u0000pace,\u0000and\u0000\n",
      "large-scale\u0000AI\u0000models\u0000are\u0000emerging\u0000as\u0000a\u0000competitive\u0000force,\u0000especially\u0000in\u0000the\u0000\n",
      "2020s.\u0000These\u0000large-scale\u0000AI\u0000models\u0000have\u0000learnt\u0000vast\u0000amounts\u0000of\u0000data\u0000based\u0000\n",
      "on\u0000a\u0000large-capacity\u0000 computational\u0000 infrastructure,\u0000 enabling\u0000comprehensive\u0000' metadata={'source': '/home/jun99/rag/pdfdata/(IS-178) 글로벌 초거대 AI 모델 현황 분석(2020~2023년)_SPRi_240819.pdf', 'page': 3}\n",
      "=========4번째 문서========================\n",
      "page_content='2020s.\u0000These\u0000large-scale\u0000AI\u0000models\u0000have\u0000learnt\u0000vast\u0000amounts\u0000of\u0000data\u0000based\u0000\n",
      "on\u0000a\u0000large-capacity\u0000 computational\u0000 infrastructure,\u0000 enabling\u0000comprehensive\u0000\n",
      "cognition,\u0000judgement,\u0000and\u0000reasoning\u0000akin\u0000to\u0000humans.\u0000Existing\u0000general\u0000AI,\u0000which\u0000\n",
      "is\u0000created\u0000by\u0000collecting\u0000and\u0000learning\u0000discrete\u0000data\u0000for\u0000a\u0000specific\u0000purpose,\u0000can\u0000\n",
      "only\u0000be\u0000applied\u0000to\u0000the\u0000tasks\u0000it\u0000has\u0000learnt,\u0000whilst\u0000large-scale\u0000AI\u0000can\u0000be\u0000applied\u0000\n",
      "to\u0000various\u0000tasks.\n",
      "This\u0000report\u0000examines\u0000the\u0000global\u0000technology\u0000trends\u0000of\u0000large-scale\u0000AI\u0000models\u0000\n",
      "released\u0000between\u00002020\u0000and\u00002023.\u0000Statistics\u0000on\u0000global\u0000large-scale\u0000AI\u0000models\u0000\n",
      "are\u0000available\u0000by\u0000release\u0000year,\u0000country,\u0000domain,\u0000task,\u0000development\u0000type,\u0000and\u0000\n",
      "organisation\u0000type.\u0000It\u0000gives\u0000special\u0000focus\u0000to\u0000the\u0000status\u0000of\u0000South\u0000Korea\u0000and\u0000\n",
      "provides\u0000policy\u0000recommendations\u0000 for\u0000the\u0000field\u0000of\u0000AI.' metadata={'source': '/home/jun99/rag/pdfdata/(IS-178) 글로벌 초거대 AI 모델 현황 분석(2020~2023년)_SPRi_240819.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "for i,split_doc in enumerate(split_docs[0:5]):\n",
    "    print(f\"========={i}번째 문서========================\\n{split_doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "https://wikidocs.net/233817\n",
    "\n",
    "https://python.langchain.com/docs/integrations/platforms/huggingface/#huggingfacebgeembeddings\n",
    "\n",
    "https://python.langchain.com/docs/integrations/text_embedding/bge_huggingface/\n",
    "\n",
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "자료들에 근거하여 bge임베딩 모델 사용 -> 제일 많이 기초모델로 사용됨\n",
    "- 궁금한게 애초에 bge자체가 한국어 특화가 아니면, 현재 좋은 성능인 bge기반 다른 모델써도 되려나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f7198e84e50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jun99/miniconda3/envs/jun/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "/home/jun99/miniconda3/envs/jun/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"} # cuda or cpu\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embedding에 넣을 때 어떤 객체로 들어가야하지?\n",
    "- 이거 때문에 한창 고생하긴했는데, 아직 잘 모르겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 과정을 확인하고 싶은데"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorDB\n",
    "Chroma Vs FAISS VS Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma -> 요걸로 써야하나..?\n",
    "https://github.com/chroma-core/chroma/tree/main/examples : chroma git\n",
    "\n",
    "https://python.langchain.com/docs/integrations/vectorstores/chroma/ : langchain chroma\n",
    "\n",
    "https://wikidocs.net/234094 : 테디노트 chroma\n",
    "- 지속성 for 웹 어플리케이션 or 서비스 구축시\n",
    "- 지속성 및 메타데이터 필터링 기능이 내장되어 있어 챗봇 대화 내역과 컨텍스트를 관리하는 데 유용하며, 인메모리 및 SQLite 등 다양한 배포 시나리오에 편리한 스토리지 옵션의 유연성을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_chroma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      3\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[1;32m      4\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m      6\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_langchain_db\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Where to save data locally, remove if not necessary\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_chroma'"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "https://python.langchain.com/docs/integrations/vectorstores/faiss/\n",
    "- 주로 대용량 벡터를 처리할 때 사용 -> GPU가속 사용 가능\n",
    "- 인덱싱 방법에 대한 세밀한 조정이 필요한 경우\n",
    "- 근데, 따로 인덱싱 저장이 되니깐 괜찮지 않나?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
